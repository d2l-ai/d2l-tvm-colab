{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following additional libraries are needed to run this\n",
    "notebook. Note that running on Colab is experimental, please report a Github\n",
    "issue if you have any problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install https://tvm-repo.s3-us-west-2.amazonaws.com/cuda10.0-llvm6.0/tvm-0.6.dev0-cp36-cp36m-linux_x86_64.whl https://tvm-repo.s3-us-west-2.amazonaws.com/cuda10.0-llvm6.0/topi-0.6.dev0-py3-none-any.whl\n",
    "!pip install git+https://github.com/d2l-ai/d2l-tvm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Multiplication\n",
    "\n",
    "Matrix Multiplication is one of the most widely operators in scientific computing and deep learning, which is typically referred to as *GEMM* (GEneral Matrix Multiply). Let's implement its computation in this section.\n",
    "\n",
    "Given $A\\in\\mathbb R^{n\\times l}$, and $B \\in\\mathbb R^{l\\times m}$, if $C=AB$ then $C \\in\\mathbb R^{n\\times m}$ and\n",
    "\n",
    "$$C_{i,j} = \\sum_{k=1}^l A_{i,k} B_{k,j}.$$\n",
    "\n",
    "The elements accessed to compute $C_{i,j}$ are illustrated in :numref:`fig_matmul_default`.\n",
    "\n",
    "![Compute $C_{x,y}$ in matrix multiplication.](http://tvm.d2l.ai/_images/matmul_default.svg)\n",
    "\n",
    ":label:`fig_matmul_default`\n",
    "\n",
    "\n",
    "The following method returns the computing expression of matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "1"
    }
   },
   "outputs": [],
   "source": [
    "import d2ltvm\n",
    "import numpy as np\n",
    "import tvm\n",
    "\n",
    "# Save to the d2ltvm package\n",
    "def matmul(n, m, l):\n",
    "    \"\"\"Return the computing expression of matrix multiplication\n",
    "    A : n x l matrix\n",
    "    B : l x m matrix\n",
    "    C : n x m matrix with C = A B\n",
    "    \"\"\"\n",
    "    k = tvm.reduce_axis((0, l), name='k')\n",
    "    A = tvm.placeholder((n, l), name='A')\n",
    "    B = tvm.placeholder((l, m), name='B')\n",
    "    C = tvm.compute((n, m),\n",
    "                    lambda x, y: tvm.sum(A[x, k] * B[k, y], axis=k),\n",
    "                    name='C')\n",
    "    return A, B, C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compile a module for a square matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "2"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "// attr [C] storage_scope = \"global\"\n",
      "allocate C[float32 * 10000]\n",
      "produce C {\n",
      "  for (x, 0, 100) {\n",
      "    for (y, 0, 100) {\n",
      "      C[((x*100) + y)] = 0f\n",
      "      for (k, 0, 100) {\n",
      "        C[((x*100) + y)] = (C[((x*100) + y)] + (A[((x*100) + k)]*B[((k*100) + y)]))\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n = 100\n",
    "A, B, C = matmul(n, n, n)\n",
    "s = tvm.create_schedule(C.op)\n",
    "print(tvm.lower(s, [A, B], simple_mode=True))\n",
    "mod = tvm.build(s, [A, B, C])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pseudo code is simply a naive 3-level nested for loop to calculate the matrix multiplication.\n",
    "\n",
    "And then we verify the results. Note that NumPy may use multi-threading to accelerate its computing, which may result in slightly different results due to the numerical error. There we use `assert_allclose` with a relative large tolerant error to test the correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    }
   },
   "outputs": [],
   "source": [
    "a, b, c = d2ltvm.get_abc((100, 100), tvm.nd.array)\n",
    "mod(a, b, c)\n",
    "np.testing.assert_allclose(np.dot(a.asnumpy(), b.asnumpy()),\n",
    "                           c.asnumpy(), atol=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- We can express the computation of matrix multiplication in TVM in one line of code.\n",
    "- The naive matrix multiplication is a 3-level nested for loop."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}